Namespace(
    num_layers=6,
    hidden_size=1024,
    num_attention_heads=16,
    vocab_size=100,
    max_sequence_length=512,
    layernorm_order="pre",
    inner_hidden_size=None,
    hidden_size_per_attention_head=None,
    model_parallel_size=1,
    skip_init=True,
    use_gpu_initialization=False,
    num_multi_query_heads=0,
    layernorm_epsilon=1e-05,
    hidden_dropout=0.1,
    attention_dropout=0.1,
    drop_path=0.0,
    make_vocab_size_divisible_by=128,
    experiment_name="finetune-visualglm-6b-eva++",
    train_iters=3000,
    batch_size=4,
    lr=0.0001,
    mode="finetune",
    seed=1234,
    zero_stage=1,
    checkpoint_activations=True,
    checkpoint_num_layers=1,
    checkpoint_skip_layers=0,
    fp16=True,
    bf16=False,
    gradient_accumulation_steps=1,
    epochs=None,
    log_interval=50,
    summary_dir="",
    save_args=False,
    lr_decay_iters=None,
    lr_decay_style="cosine",
    lr_decay_ratio=0.1,
    warmup=0.02,
    weight_decay=0.01,
    save="./checkpoints",
    load=None,
    save_interval=1000,
    no_save_rng=False,
    no_load_rng=False,
    resume_dataloader=True,
    distributed_backend="nccl",
    local_rank=0,
    exit_interval=None,
    eval_batch_size=8,
    eval_iters=10,
    eval_interval=10,
    strict_eval=False,
    train_data=["/home/qianq/data/COV-CTR/train.json"],
    train_data_weights=None,
    iterable_dataset=False,
    valid_data=["/home/qianq/data/COV-CTR/eval.json"],
    test_data=None,
    split="1",
    num_workers=1,
    block_size=10000,
    prefetch_factor=4,
    tokenizer_type="fake",
    temperature=1.0,
    top_p=0.0,
    top_k=0,
    num_beams=1,
    length_penalty=0.0,
    no_repeat_ngram_size=0,
    min_tgt_length=0,
    out_seq_length=256,
    input_source="interactive",
    output_path="./samples",
    with_id=False,
    max_inference_batch_size=12,
    device="cpu",
    deepspeed=True,
    deepspeed_config={
        "train_micro_batch_size_per_gpu": 4,
        "gradient_accumulation_steps": 1,
        "gradient_clipping": 0.1,
        "zero_optimization": {
            "stage": 1,
            "cpu_offload": False,
            "contiguous_gradients": False,
            "overlap_comm": True,
            "reduce_scatter": True,
            "reduce_bucket_size": 40000000.0,
            "allgather_bucket_size": 100000000.0,
            "load_from_fp32_weights": False,
        },
        "zero_allow_untested_optimizer": True,
        "fp16": {
            "enabled": True,
            "loss_scale": 0,
            "loss_scale_window": 400,
            "hysteresis": 2,
            "min_loss_scale": 1,
        },
        "bf16": {"enabled": False},
        "optimizer": {
            "type": "Adam",
            "params": {
                "lr": 0.0001,
                "betas": [0.9, 0.95],
                "eps": 1e-08,
                "weight_decay": 0.01,
            },
        },
        "activation_checkpointing": {
            "partition_activations": False,
            "contiguous_memory_optimization": False,
        },
        "wall_clock_breakdown": False,
    },
    deepscale=False,
    deepscale_config=None,
    deepspeed_mpi=False,
    cuda=True,
    rank=0,
    world_size=1,
    deepspeed_activation_checkpointing=True,
    master_ip="localhost",
    master_port="45473",
    max_source_length=64,
    max_target_length=256,
    ignore_pad_token_for_loss=True,
    source_prefix="",
    pre_seq_len=8,
    lora_rank=10,
    use_ptuning=False,
    use_lora=False,
    use_qlora=False,
    layer_range=None,
    use_adapter=False,
    adapter_hidden=128,
    adapter_num_layers=28,
    use_freeze=False,
    unfreeze_layers="",
    train_qformer=True,
    image_length=32,
    eva_args={},
    qformer_args={},
    bos_token_id=None,
    mask_token_id=None,
    gmask_token_id=None,
    pad_token_id=None,
)
