{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-30 16:10:20,880] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qianq/mycodes/VisualGLM-6B/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[2023-11-30 16:10:22,216] [WARNING] Failed to load bitsandbytes:No module named 'bitsandbytes'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from model.visualglm import VisualGLMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-30 16:10:22,264] [INFO] building VisualGLMModel model ...\n",
      "[2023-11-30 16:10:22,293] [INFO] [RANK 0] > initializing model parallel with size 1\n",
      "[2023-11-30 16:10:22,294] [INFO] [RANK 0] You are using model-only mode.\n",
      "For torch.distributed users or loading model parallel models, set environment variables RANK, WORLD_SIZE and LOCAL_RANK.\n",
      "/home/qianq/mycodes/VisualGLM-6B/venv/lib/python3.10/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "[2023-11-30 16:10:29,962] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 7802193408\n",
      "[2023-11-30 16:10:31,225] [INFO] [RANK 0] global rank 0 is loading checkpoint /home/qianq/.sat_models/visualglm-6b/1/mp_rank_00_model_states.pt\n",
      "[2023-11-30 16:10:38,085] [INFO] [RANK 0] Will continue but found unexpected_keys! Check whether you are loading correct checkpoints: ['transformer.position_embeddings.weight'].\n",
      "[2023-11-30 16:10:38,090] [INFO] [RANK 0] > successfully loaded /home/qianq/.sat_models/visualglm-6b/1/mp_rank_00_model_states.pt\n"
     ]
    }
   ],
   "source": [
    "model, model_args = VisualGLMModel.from_pretrained(\n",
    "    name=\"visualglm-6b\",\n",
    "    args=argparse.Namespace(\n",
    "        fp16=True,\n",
    "        skip_init=True,\n",
    "        use_gpu_initialization=True,\n",
    "        device='cuda',\n",
    "    )\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_children_struct(model):\n",
    "    for item in model.named_children():\n",
    "        print('-----------------')\n",
    "        print(item[0])\n",
    "        print(item[1])\n",
    "        print('-----------------')\n",
    "\n",
    "def count_model_parameters(model):\n",
    "    total = sum([param.nelement() for param in model.parameters()])\n",
    "    print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "mixins\n",
      "ModuleDict(\n",
      "  (chatglm-final): ChatGLMFinalMixin(\n",
      "    (lm_head): ColumnParallelLinear()\n",
      "  )\n",
      "  (chatglm-attn): ChatGLMAttnMixin(\n",
      "    (rotary_emb): RotaryEmbedding()\n",
      "  )\n",
      "  (chatglm-layer): ChatGLMLayerMixin()\n",
      "  (eva): ImageMixin(\n",
      "    (model): BLIP2(\n",
      "      (vit): EVAViT(\n",
      "        (mixins): ModuleDict(\n",
      "          (patch_embedding): ImagePatchEmbeddingMixin(\n",
      "            (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "          )\n",
      "          (pos_embedding): InterpolatedPositionEmbeddingMixin()\n",
      "          (cls): LNFinalyMixin(\n",
      "            (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (transformer): BaseTransformer(\n",
      "          (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (word_embeddings): Embedding(1, 1408)\n",
      "          (position_embeddings): Embedding(257, 1408)\n",
      "          (layers): ModuleList(\n",
      "            (0-38): 39 x BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (qformer): QFormer(\n",
      "        (mixins): ModuleDict()\n",
      "        (transformer): BaseTransformer(\n",
      "          (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (word_embeddings): Embedding(32, 768)\n",
      "          (position_embeddings): None\n",
      "          (layers): ModuleList(\n",
      "            (0): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (final_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (glm_proj): Linear(in_features=768, out_features=4096, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "transformer\n",
      "BaseTransformer(\n",
      "  (embedding_dropout): Dropout(p=0, inplace=False)\n",
      "  (word_embeddings): VocabParallelEmbedding()\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x BaseTransformerLayer(\n",
      "      (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): SelfAttention(\n",
      "        (query_key_value): ColumnParallelLinear()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "        (dense): RowParallelLinear()\n",
      "        (output_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (dense_h_to_4h): ColumnParallelLinear()\n",
      "        (dense_4h_to_h): RowParallelLinear()\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "show_children_struct(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "chatglm-final\n",
      "ChatGLMFinalMixin(\n",
      "  (lm_head): ColumnParallelLinear()\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "chatglm-attn\n",
      "ChatGLMAttnMixin(\n",
      "  (rotary_emb): RotaryEmbedding()\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "chatglm-layer\n",
      "ChatGLMLayerMixin()\n",
      "-----------------\n",
      "-----------------\n",
      "eva\n",
      "ImageMixin(\n",
      "  (model): BLIP2(\n",
      "    (vit): EVAViT(\n",
      "      (mixins): ModuleDict(\n",
      "        (patch_embedding): ImagePatchEmbeddingMixin(\n",
      "          (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "        )\n",
      "        (pos_embedding): InterpolatedPositionEmbeddingMixin()\n",
      "        (cls): LNFinalyMixin(\n",
      "          (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer): BaseTransformer(\n",
      "        (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (word_embeddings): Embedding(1, 1408)\n",
      "        (position_embeddings): Embedding(257, 1408)\n",
      "        (layers): ModuleList(\n",
      "          (0-38): 39 x BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (qformer): QFormer(\n",
      "      (mixins): ModuleDict()\n",
      "      (transformer): BaseTransformer(\n",
      "        (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (word_embeddings): Embedding(32, 768)\n",
      "        (position_embeddings): None\n",
      "        (layers): ModuleList(\n",
      "          (0): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (glm_proj): Linear(in_features=768, out_features=4096, bias=True)\n",
      "  )\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "mixins = model.mixins\n",
    "show_children_struct(mixins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "model\n",
      "BLIP2(\n",
      "  (vit): EVAViT(\n",
      "    (mixins): ModuleDict(\n",
      "      (patch_embedding): ImagePatchEmbeddingMixin(\n",
      "        (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "      )\n",
      "      (pos_embedding): InterpolatedPositionEmbeddingMixin()\n",
      "      (cls): LNFinalyMixin(\n",
      "        (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (transformer): BaseTransformer(\n",
      "      (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (word_embeddings): Embedding(1, 1408)\n",
      "      (position_embeddings): Embedding(257, 1408)\n",
      "      (layers): ModuleList(\n",
      "        (0-38): 39 x BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qformer): QFormer(\n",
      "    (mixins): ModuleDict()\n",
      "    (transformer): BaseTransformer(\n",
      "      (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (word_embeddings): Embedding(32, 768)\n",
      "      (position_embeddings): None\n",
      "      (layers): ModuleList(\n",
      "        (0): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (glm_proj): Linear(in_features=768, out_features=4096, bias=True)\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "eva = model.mixins.eva\n",
    "show_children_struct(eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 1094.26M\n"
     ]
    }
   ],
   "source": [
    "count_model_parameters(eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 985.95M\n",
      "Number of parameter: 105.16M\n"
     ]
    }
   ],
   "source": [
    "vit = eva.model.vit\n",
    "qformer = eva.model.qformer\n",
    "count_model_parameters(vit)\n",
    "count_model_parameters(qformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "mixins\n",
      "ModuleDict(\n",
      "  (patch_embedding): ImagePatchEmbeddingMixin(\n",
      "    (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "  )\n",
      "  (pos_embedding): InterpolatedPositionEmbeddingMixin()\n",
      "  (cls): LNFinalyMixin(\n",
      "    (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "transformer\n",
      "BaseTransformer(\n",
      "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (word_embeddings): Embedding(1, 1408)\n",
      "  (position_embeddings): Embedding(257, 1408)\n",
      "  (layers): ModuleList(\n",
      "    (0-38): 39 x BaseTransformerLayer(\n",
      "      (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "      (attention): SelfAttention(\n",
      "        (query_key_value): ColumnParallelLinear()\n",
      "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (dense): RowParallelLinear()\n",
      "        (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (dense_h_to_4h): ColumnParallelLinear()\n",
      "        (dense_4h_to_h): RowParallelLinear()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "show_children_struct(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "embedding_dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "-----------------\n",
      "-----------------\n",
      "word_embeddings\n",
      "Embedding(1, 1408)\n",
      "-----------------\n",
      "-----------------\n",
      "position_embeddings\n",
      "Embedding(257, 1408)\n",
      "-----------------\n",
      "-----------------\n",
      "layers\n",
      "ModuleList(\n",
      "  (0-38): 39 x BaseTransformerLayer(\n",
      "    (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "    (attention): SelfAttention(\n",
      "      (query_key_value): ColumnParallelLinear()\n",
      "      (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (dense): RowParallelLinear()\n",
      "      (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLP(\n",
      "      (dense_h_to_4h): ColumnParallelLinear()\n",
      "      (dense_4h_to_h): RowParallelLinear()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "vit_transformer = vit.transformer\n",
    "show_children_struct(vit_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeddings.weight\n",
      "position_embeddings.weight\n",
      "layers.0.input_layernorm.weight\n",
      "layers.0.input_layernorm.bias\n",
      "layers.0.attention.query_key_value.weight\n",
      "layers.0.attention.query_key_value.bias\n",
      "layers.0.attention.dense.weight\n",
      "layers.0.attention.dense.bias\n",
      "layers.0.post_attention_layernorm.weight\n",
      "layers.0.post_attention_layernorm.bias\n",
      "layers.0.mlp.dense_h_to_4h.weight\n",
      "layers.0.mlp.dense_h_to_4h.bias\n",
      "layers.0.mlp.dense_4h_to_h.weight\n",
      "layers.0.mlp.dense_4h_to_h.bias\n",
      "layers.1.input_layernorm.weight\n",
      "layers.1.input_layernorm.bias\n",
      "layers.1.attention.query_key_value.weight\n",
      "layers.1.attention.query_key_value.bias\n",
      "layers.1.attention.dense.weight\n",
      "layers.1.attention.dense.bias\n",
      "layers.1.post_attention_layernorm.weight\n",
      "layers.1.post_attention_layernorm.bias\n",
      "layers.1.mlp.dense_h_to_4h.weight\n",
      "layers.1.mlp.dense_h_to_4h.bias\n",
      "layers.1.mlp.dense_4h_to_h.weight\n",
      "layers.1.mlp.dense_4h_to_h.bias\n",
      "layers.2.input_layernorm.weight\n",
      "layers.2.input_layernorm.bias\n",
      "layers.2.attention.query_key_value.weight\n",
      "layers.2.attention.query_key_value.bias\n",
      "layers.2.attention.dense.weight\n",
      "layers.2.attention.dense.bias\n",
      "layers.2.post_attention_layernorm.weight\n",
      "layers.2.post_attention_layernorm.bias\n",
      "layers.2.mlp.dense_h_to_4h.weight\n",
      "layers.2.mlp.dense_h_to_4h.bias\n",
      "layers.2.mlp.dense_4h_to_h.weight\n",
      "layers.2.mlp.dense_4h_to_h.bias\n",
      "layers.3.input_layernorm.weight\n",
      "layers.3.input_layernorm.bias\n",
      "layers.3.attention.query_key_value.weight\n",
      "layers.3.attention.query_key_value.bias\n",
      "layers.3.attention.dense.weight\n",
      "layers.3.attention.dense.bias\n",
      "layers.3.post_attention_layernorm.weight\n",
      "layers.3.post_attention_layernorm.bias\n",
      "layers.3.mlp.dense_h_to_4h.weight\n",
      "layers.3.mlp.dense_h_to_4h.bias\n",
      "layers.3.mlp.dense_4h_to_h.weight\n",
      "layers.3.mlp.dense_4h_to_h.bias\n",
      "layers.4.input_layernorm.weight\n",
      "layers.4.input_layernorm.bias\n",
      "layers.4.attention.query_key_value.weight\n",
      "layers.4.attention.query_key_value.bias\n",
      "layers.4.attention.dense.weight\n",
      "layers.4.attention.dense.bias\n",
      "layers.4.post_attention_layernorm.weight\n",
      "layers.4.post_attention_layernorm.bias\n",
      "layers.4.mlp.dense_h_to_4h.weight\n",
      "layers.4.mlp.dense_h_to_4h.bias\n",
      "layers.4.mlp.dense_4h_to_h.weight\n",
      "layers.4.mlp.dense_4h_to_h.bias\n",
      "layers.5.input_layernorm.weight\n",
      "layers.5.input_layernorm.bias\n",
      "layers.5.attention.query_key_value.weight\n",
      "layers.5.attention.query_key_value.bias\n",
      "layers.5.attention.dense.weight\n",
      "layers.5.attention.dense.bias\n",
      "layers.5.post_attention_layernorm.weight\n",
      "layers.5.post_attention_layernorm.bias\n",
      "layers.5.mlp.dense_h_to_4h.weight\n",
      "layers.5.mlp.dense_h_to_4h.bias\n",
      "layers.5.mlp.dense_4h_to_h.weight\n",
      "layers.5.mlp.dense_4h_to_h.bias\n",
      "layers.6.input_layernorm.weight\n",
      "layers.6.input_layernorm.bias\n",
      "layers.6.attention.query_key_value.weight\n",
      "layers.6.attention.query_key_value.bias\n",
      "layers.6.attention.dense.weight\n",
      "layers.6.attention.dense.bias\n",
      "layers.6.post_attention_layernorm.weight\n",
      "layers.6.post_attention_layernorm.bias\n",
      "layers.6.mlp.dense_h_to_4h.weight\n",
      "layers.6.mlp.dense_h_to_4h.bias\n",
      "layers.6.mlp.dense_4h_to_h.weight\n",
      "layers.6.mlp.dense_4h_to_h.bias\n",
      "layers.7.input_layernorm.weight\n",
      "layers.7.input_layernorm.bias\n",
      "layers.7.attention.query_key_value.weight\n",
      "layers.7.attention.query_key_value.bias\n",
      "layers.7.attention.dense.weight\n",
      "layers.7.attention.dense.bias\n",
      "layers.7.post_attention_layernorm.weight\n",
      "layers.7.post_attention_layernorm.bias\n",
      "layers.7.mlp.dense_h_to_4h.weight\n",
      "layers.7.mlp.dense_h_to_4h.bias\n",
      "layers.7.mlp.dense_4h_to_h.weight\n",
      "layers.7.mlp.dense_4h_to_h.bias\n",
      "layers.8.input_layernorm.weight\n",
      "layers.8.input_layernorm.bias\n",
      "layers.8.attention.query_key_value.weight\n",
      "layers.8.attention.query_key_value.bias\n",
      "layers.8.attention.dense.weight\n",
      "layers.8.attention.dense.bias\n",
      "layers.8.post_attention_layernorm.weight\n",
      "layers.8.post_attention_layernorm.bias\n",
      "layers.8.mlp.dense_h_to_4h.weight\n",
      "layers.8.mlp.dense_h_to_4h.bias\n",
      "layers.8.mlp.dense_4h_to_h.weight\n",
      "layers.8.mlp.dense_4h_to_h.bias\n",
      "layers.9.input_layernorm.weight\n",
      "layers.9.input_layernorm.bias\n",
      "layers.9.attention.query_key_value.weight\n",
      "layers.9.attention.query_key_value.bias\n",
      "layers.9.attention.dense.weight\n",
      "layers.9.attention.dense.bias\n",
      "layers.9.post_attention_layernorm.weight\n",
      "layers.9.post_attention_layernorm.bias\n",
      "layers.9.mlp.dense_h_to_4h.weight\n",
      "layers.9.mlp.dense_h_to_4h.bias\n",
      "layers.9.mlp.dense_4h_to_h.weight\n",
      "layers.9.mlp.dense_4h_to_h.bias\n",
      "layers.10.input_layernorm.weight\n",
      "layers.10.input_layernorm.bias\n",
      "layers.10.attention.query_key_value.weight\n",
      "layers.10.attention.query_key_value.bias\n",
      "layers.10.attention.dense.weight\n",
      "layers.10.attention.dense.bias\n",
      "layers.10.post_attention_layernorm.weight\n",
      "layers.10.post_attention_layernorm.bias\n",
      "layers.10.mlp.dense_h_to_4h.weight\n",
      "layers.10.mlp.dense_h_to_4h.bias\n",
      "layers.10.mlp.dense_4h_to_h.weight\n",
      "layers.10.mlp.dense_4h_to_h.bias\n",
      "layers.11.input_layernorm.weight\n",
      "layers.11.input_layernorm.bias\n",
      "layers.11.attention.query_key_value.weight\n",
      "layers.11.attention.query_key_value.bias\n",
      "layers.11.attention.dense.weight\n",
      "layers.11.attention.dense.bias\n",
      "layers.11.post_attention_layernorm.weight\n",
      "layers.11.post_attention_layernorm.bias\n",
      "layers.11.mlp.dense_h_to_4h.weight\n",
      "layers.11.mlp.dense_h_to_4h.bias\n",
      "layers.11.mlp.dense_4h_to_h.weight\n",
      "layers.11.mlp.dense_4h_to_h.bias\n",
      "layers.12.input_layernorm.weight\n",
      "layers.12.input_layernorm.bias\n",
      "layers.12.attention.query_key_value.weight\n",
      "layers.12.attention.query_key_value.bias\n",
      "layers.12.attention.dense.weight\n",
      "layers.12.attention.dense.bias\n",
      "layers.12.post_attention_layernorm.weight\n",
      "layers.12.post_attention_layernorm.bias\n",
      "layers.12.mlp.dense_h_to_4h.weight\n",
      "layers.12.mlp.dense_h_to_4h.bias\n",
      "layers.12.mlp.dense_4h_to_h.weight\n",
      "layers.12.mlp.dense_4h_to_h.bias\n",
      "layers.13.input_layernorm.weight\n",
      "layers.13.input_layernorm.bias\n",
      "layers.13.attention.query_key_value.weight\n",
      "layers.13.attention.query_key_value.bias\n",
      "layers.13.attention.dense.weight\n",
      "layers.13.attention.dense.bias\n",
      "layers.13.post_attention_layernorm.weight\n",
      "layers.13.post_attention_layernorm.bias\n",
      "layers.13.mlp.dense_h_to_4h.weight\n",
      "layers.13.mlp.dense_h_to_4h.bias\n",
      "layers.13.mlp.dense_4h_to_h.weight\n",
      "layers.13.mlp.dense_4h_to_h.bias\n",
      "layers.14.input_layernorm.weight\n",
      "layers.14.input_layernorm.bias\n",
      "layers.14.attention.query_key_value.weight\n",
      "layers.14.attention.query_key_value.bias\n",
      "layers.14.attention.dense.weight\n",
      "layers.14.attention.dense.bias\n",
      "layers.14.post_attention_layernorm.weight\n",
      "layers.14.post_attention_layernorm.bias\n",
      "layers.14.mlp.dense_h_to_4h.weight\n",
      "layers.14.mlp.dense_h_to_4h.bias\n",
      "layers.14.mlp.dense_4h_to_h.weight\n",
      "layers.14.mlp.dense_4h_to_h.bias\n",
      "layers.15.input_layernorm.weight\n",
      "layers.15.input_layernorm.bias\n",
      "layers.15.attention.query_key_value.weight\n",
      "layers.15.attention.query_key_value.bias\n",
      "layers.15.attention.dense.weight\n",
      "layers.15.attention.dense.bias\n",
      "layers.15.post_attention_layernorm.weight\n",
      "layers.15.post_attention_layernorm.bias\n",
      "layers.15.mlp.dense_h_to_4h.weight\n",
      "layers.15.mlp.dense_h_to_4h.bias\n",
      "layers.15.mlp.dense_4h_to_h.weight\n",
      "layers.15.mlp.dense_4h_to_h.bias\n",
      "layers.16.input_layernorm.weight\n",
      "layers.16.input_layernorm.bias\n",
      "layers.16.attention.query_key_value.weight\n",
      "layers.16.attention.query_key_value.bias\n",
      "layers.16.attention.dense.weight\n",
      "layers.16.attention.dense.bias\n",
      "layers.16.post_attention_layernorm.weight\n",
      "layers.16.post_attention_layernorm.bias\n",
      "layers.16.mlp.dense_h_to_4h.weight\n",
      "layers.16.mlp.dense_h_to_4h.bias\n",
      "layers.16.mlp.dense_4h_to_h.weight\n",
      "layers.16.mlp.dense_4h_to_h.bias\n",
      "layers.17.input_layernorm.weight\n",
      "layers.17.input_layernorm.bias\n",
      "layers.17.attention.query_key_value.weight\n",
      "layers.17.attention.query_key_value.bias\n",
      "layers.17.attention.dense.weight\n",
      "layers.17.attention.dense.bias\n",
      "layers.17.post_attention_layernorm.weight\n",
      "layers.17.post_attention_layernorm.bias\n",
      "layers.17.mlp.dense_h_to_4h.weight\n",
      "layers.17.mlp.dense_h_to_4h.bias\n",
      "layers.17.mlp.dense_4h_to_h.weight\n",
      "layers.17.mlp.dense_4h_to_h.bias\n",
      "layers.18.input_layernorm.weight\n",
      "layers.18.input_layernorm.bias\n",
      "layers.18.attention.query_key_value.weight\n",
      "layers.18.attention.query_key_value.bias\n",
      "layers.18.attention.dense.weight\n",
      "layers.18.attention.dense.bias\n",
      "layers.18.post_attention_layernorm.weight\n",
      "layers.18.post_attention_layernorm.bias\n",
      "layers.18.mlp.dense_h_to_4h.weight\n",
      "layers.18.mlp.dense_h_to_4h.bias\n",
      "layers.18.mlp.dense_4h_to_h.weight\n",
      "layers.18.mlp.dense_4h_to_h.bias\n",
      "layers.19.input_layernorm.weight\n",
      "layers.19.input_layernorm.bias\n",
      "layers.19.attention.query_key_value.weight\n",
      "layers.19.attention.query_key_value.bias\n",
      "layers.19.attention.dense.weight\n",
      "layers.19.attention.dense.bias\n",
      "layers.19.post_attention_layernorm.weight\n",
      "layers.19.post_attention_layernorm.bias\n",
      "layers.19.mlp.dense_h_to_4h.weight\n",
      "layers.19.mlp.dense_h_to_4h.bias\n",
      "layers.19.mlp.dense_4h_to_h.weight\n",
      "layers.19.mlp.dense_4h_to_h.bias\n",
      "layers.20.input_layernorm.weight\n",
      "layers.20.input_layernorm.bias\n",
      "layers.20.attention.query_key_value.weight\n",
      "layers.20.attention.query_key_value.bias\n",
      "layers.20.attention.dense.weight\n",
      "layers.20.attention.dense.bias\n",
      "layers.20.post_attention_layernorm.weight\n",
      "layers.20.post_attention_layernorm.bias\n",
      "layers.20.mlp.dense_h_to_4h.weight\n",
      "layers.20.mlp.dense_h_to_4h.bias\n",
      "layers.20.mlp.dense_4h_to_h.weight\n",
      "layers.20.mlp.dense_4h_to_h.bias\n",
      "layers.21.input_layernorm.weight\n",
      "layers.21.input_layernorm.bias\n",
      "layers.21.attention.query_key_value.weight\n",
      "layers.21.attention.query_key_value.bias\n",
      "layers.21.attention.dense.weight\n",
      "layers.21.attention.dense.bias\n",
      "layers.21.post_attention_layernorm.weight\n",
      "layers.21.post_attention_layernorm.bias\n",
      "layers.21.mlp.dense_h_to_4h.weight\n",
      "layers.21.mlp.dense_h_to_4h.bias\n",
      "layers.21.mlp.dense_4h_to_h.weight\n",
      "layers.21.mlp.dense_4h_to_h.bias\n",
      "layers.22.input_layernorm.weight\n",
      "layers.22.input_layernorm.bias\n",
      "layers.22.attention.query_key_value.weight\n",
      "layers.22.attention.query_key_value.bias\n",
      "layers.22.attention.dense.weight\n",
      "layers.22.attention.dense.bias\n",
      "layers.22.post_attention_layernorm.weight\n",
      "layers.22.post_attention_layernorm.bias\n",
      "layers.22.mlp.dense_h_to_4h.weight\n",
      "layers.22.mlp.dense_h_to_4h.bias\n",
      "layers.22.mlp.dense_4h_to_h.weight\n",
      "layers.22.mlp.dense_4h_to_h.bias\n",
      "layers.23.input_layernorm.weight\n",
      "layers.23.input_layernorm.bias\n",
      "layers.23.attention.query_key_value.weight\n",
      "layers.23.attention.query_key_value.bias\n",
      "layers.23.attention.dense.weight\n",
      "layers.23.attention.dense.bias\n",
      "layers.23.post_attention_layernorm.weight\n",
      "layers.23.post_attention_layernorm.bias\n",
      "layers.23.mlp.dense_h_to_4h.weight\n",
      "layers.23.mlp.dense_h_to_4h.bias\n",
      "layers.23.mlp.dense_4h_to_h.weight\n",
      "layers.23.mlp.dense_4h_to_h.bias\n",
      "layers.24.input_layernorm.weight\n",
      "layers.24.input_layernorm.bias\n",
      "layers.24.attention.query_key_value.weight\n",
      "layers.24.attention.query_key_value.bias\n",
      "layers.24.attention.dense.weight\n",
      "layers.24.attention.dense.bias\n",
      "layers.24.post_attention_layernorm.weight\n",
      "layers.24.post_attention_layernorm.bias\n",
      "layers.24.mlp.dense_h_to_4h.weight\n",
      "layers.24.mlp.dense_h_to_4h.bias\n",
      "layers.24.mlp.dense_4h_to_h.weight\n",
      "layers.24.mlp.dense_4h_to_h.bias\n",
      "layers.25.input_layernorm.weight\n",
      "layers.25.input_layernorm.bias\n",
      "layers.25.attention.query_key_value.weight\n",
      "layers.25.attention.query_key_value.bias\n",
      "layers.25.attention.dense.weight\n",
      "layers.25.attention.dense.bias\n",
      "layers.25.post_attention_layernorm.weight\n",
      "layers.25.post_attention_layernorm.bias\n",
      "layers.25.mlp.dense_h_to_4h.weight\n",
      "layers.25.mlp.dense_h_to_4h.bias\n",
      "layers.25.mlp.dense_4h_to_h.weight\n",
      "layers.25.mlp.dense_4h_to_h.bias\n",
      "layers.26.input_layernorm.weight\n",
      "layers.26.input_layernorm.bias\n",
      "layers.26.attention.query_key_value.weight\n",
      "layers.26.attention.query_key_value.bias\n",
      "layers.26.attention.dense.weight\n",
      "layers.26.attention.dense.bias\n",
      "layers.26.post_attention_layernorm.weight\n",
      "layers.26.post_attention_layernorm.bias\n",
      "layers.26.mlp.dense_h_to_4h.weight\n",
      "layers.26.mlp.dense_h_to_4h.bias\n",
      "layers.26.mlp.dense_4h_to_h.weight\n",
      "layers.26.mlp.dense_4h_to_h.bias\n",
      "layers.27.input_layernorm.weight\n",
      "layers.27.input_layernorm.bias\n",
      "layers.27.attention.query_key_value.weight\n",
      "layers.27.attention.query_key_value.bias\n",
      "layers.27.attention.dense.weight\n",
      "layers.27.attention.dense.bias\n",
      "layers.27.post_attention_layernorm.weight\n",
      "layers.27.post_attention_layernorm.bias\n",
      "layers.27.mlp.dense_h_to_4h.weight\n",
      "layers.27.mlp.dense_h_to_4h.bias\n",
      "layers.27.mlp.dense_4h_to_h.weight\n",
      "layers.27.mlp.dense_4h_to_h.bias\n",
      "layers.28.input_layernorm.weight\n",
      "layers.28.input_layernorm.bias\n",
      "layers.28.attention.query_key_value.weight\n",
      "layers.28.attention.query_key_value.bias\n",
      "layers.28.attention.dense.weight\n",
      "layers.28.attention.dense.bias\n",
      "layers.28.post_attention_layernorm.weight\n",
      "layers.28.post_attention_layernorm.bias\n",
      "layers.28.mlp.dense_h_to_4h.weight\n",
      "layers.28.mlp.dense_h_to_4h.bias\n",
      "layers.28.mlp.dense_4h_to_h.weight\n",
      "layers.28.mlp.dense_4h_to_h.bias\n",
      "layers.29.input_layernorm.weight\n",
      "layers.29.input_layernorm.bias\n",
      "layers.29.attention.query_key_value.weight\n",
      "layers.29.attention.query_key_value.bias\n",
      "layers.29.attention.dense.weight\n",
      "layers.29.attention.dense.bias\n",
      "layers.29.post_attention_layernorm.weight\n",
      "layers.29.post_attention_layernorm.bias\n",
      "layers.29.mlp.dense_h_to_4h.weight\n",
      "layers.29.mlp.dense_h_to_4h.bias\n",
      "layers.29.mlp.dense_4h_to_h.weight\n",
      "layers.29.mlp.dense_4h_to_h.bias\n",
      "layers.30.input_layernorm.weight\n",
      "layers.30.input_layernorm.bias\n",
      "layers.30.attention.query_key_value.weight\n",
      "layers.30.attention.query_key_value.bias\n",
      "layers.30.attention.dense.weight\n",
      "layers.30.attention.dense.bias\n",
      "layers.30.post_attention_layernorm.weight\n",
      "layers.30.post_attention_layernorm.bias\n",
      "layers.30.mlp.dense_h_to_4h.weight\n",
      "layers.30.mlp.dense_h_to_4h.bias\n",
      "layers.30.mlp.dense_4h_to_h.weight\n",
      "layers.30.mlp.dense_4h_to_h.bias\n",
      "layers.31.input_layernorm.weight\n",
      "layers.31.input_layernorm.bias\n",
      "layers.31.attention.query_key_value.weight\n",
      "layers.31.attention.query_key_value.bias\n",
      "layers.31.attention.dense.weight\n",
      "layers.31.attention.dense.bias\n",
      "layers.31.post_attention_layernorm.weight\n",
      "layers.31.post_attention_layernorm.bias\n",
      "layers.31.mlp.dense_h_to_4h.weight\n",
      "layers.31.mlp.dense_h_to_4h.bias\n",
      "layers.31.mlp.dense_4h_to_h.weight\n",
      "layers.31.mlp.dense_4h_to_h.bias\n",
      "layers.32.input_layernorm.weight\n",
      "layers.32.input_layernorm.bias\n",
      "layers.32.attention.query_key_value.weight\n",
      "layers.32.attention.query_key_value.bias\n",
      "layers.32.attention.dense.weight\n",
      "layers.32.attention.dense.bias\n",
      "layers.32.post_attention_layernorm.weight\n",
      "layers.32.post_attention_layernorm.bias\n",
      "layers.32.mlp.dense_h_to_4h.weight\n",
      "layers.32.mlp.dense_h_to_4h.bias\n",
      "layers.32.mlp.dense_4h_to_h.weight\n",
      "layers.32.mlp.dense_4h_to_h.bias\n",
      "layers.33.input_layernorm.weight\n",
      "layers.33.input_layernorm.bias\n",
      "layers.33.attention.query_key_value.weight\n",
      "layers.33.attention.query_key_value.bias\n",
      "layers.33.attention.dense.weight\n",
      "layers.33.attention.dense.bias\n",
      "layers.33.post_attention_layernorm.weight\n",
      "layers.33.post_attention_layernorm.bias\n",
      "layers.33.mlp.dense_h_to_4h.weight\n",
      "layers.33.mlp.dense_h_to_4h.bias\n",
      "layers.33.mlp.dense_4h_to_h.weight\n",
      "layers.33.mlp.dense_4h_to_h.bias\n",
      "layers.34.input_layernorm.weight\n",
      "layers.34.input_layernorm.bias\n",
      "layers.34.attention.query_key_value.weight\n",
      "layers.34.attention.query_key_value.bias\n",
      "layers.34.attention.dense.weight\n",
      "layers.34.attention.dense.bias\n",
      "layers.34.post_attention_layernorm.weight\n",
      "layers.34.post_attention_layernorm.bias\n",
      "layers.34.mlp.dense_h_to_4h.weight\n",
      "layers.34.mlp.dense_h_to_4h.bias\n",
      "layers.34.mlp.dense_4h_to_h.weight\n",
      "layers.34.mlp.dense_4h_to_h.bias\n",
      "layers.35.input_layernorm.weight\n",
      "layers.35.input_layernorm.bias\n",
      "layers.35.attention.query_key_value.weight\n",
      "layers.35.attention.query_key_value.bias\n",
      "layers.35.attention.dense.weight\n",
      "layers.35.attention.dense.bias\n",
      "layers.35.post_attention_layernorm.weight\n",
      "layers.35.post_attention_layernorm.bias\n",
      "layers.35.mlp.dense_h_to_4h.weight\n",
      "layers.35.mlp.dense_h_to_4h.bias\n",
      "layers.35.mlp.dense_4h_to_h.weight\n",
      "layers.35.mlp.dense_4h_to_h.bias\n",
      "layers.36.input_layernorm.weight\n",
      "layers.36.input_layernorm.bias\n",
      "layers.36.attention.query_key_value.weight\n",
      "layers.36.attention.query_key_value.bias\n",
      "layers.36.attention.dense.weight\n",
      "layers.36.attention.dense.bias\n",
      "layers.36.post_attention_layernorm.weight\n",
      "layers.36.post_attention_layernorm.bias\n",
      "layers.36.mlp.dense_h_to_4h.weight\n",
      "layers.36.mlp.dense_h_to_4h.bias\n",
      "layers.36.mlp.dense_4h_to_h.weight\n",
      "layers.36.mlp.dense_4h_to_h.bias\n",
      "layers.37.input_layernorm.weight\n",
      "layers.37.input_layernorm.bias\n",
      "layers.37.attention.query_key_value.weight\n",
      "layers.37.attention.query_key_value.bias\n",
      "layers.37.attention.dense.weight\n",
      "layers.37.attention.dense.bias\n",
      "layers.37.post_attention_layernorm.weight\n",
      "layers.37.post_attention_layernorm.bias\n",
      "layers.37.mlp.dense_h_to_4h.weight\n",
      "layers.37.mlp.dense_h_to_4h.bias\n",
      "layers.37.mlp.dense_4h_to_h.weight\n",
      "layers.37.mlp.dense_4h_to_h.bias\n",
      "layers.38.input_layernorm.weight\n",
      "layers.38.input_layernorm.bias\n",
      "layers.38.attention.query_key_value.weight\n",
      "layers.38.attention.query_key_value.bias\n",
      "layers.38.attention.dense.weight\n",
      "layers.38.attention.dense.bias\n",
      "layers.38.post_attention_layernorm.weight\n",
      "layers.38.post_attention_layernorm.bias\n",
      "layers.38.mlp.dense_h_to_4h.weight\n",
      "layers.38.mlp.dense_h_to_4h.bias\n",
      "layers.38.mlp.dense_4h_to_h.weight\n",
      "layers.38.mlp.dense_4h_to_h.bias\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "for n, w in vit_transformer.named_parameters():\n",
    "    print(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
