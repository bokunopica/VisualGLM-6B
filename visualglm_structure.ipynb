{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-24 15:33:05,189] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qianq/mycodes/VisualGLM-6B/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from model.visualglm import VisualGLMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-24 15:33:06,245] [INFO] building VisualGLMModel model ...\n",
      "[2023-10-24 15:33:06,279] [INFO] [RANK 0] > initializing model parallel with size 1\n",
      "[2023-10-24 15:33:06,282] [INFO] [RANK 0] You are using model-only mode.\n",
      "For torch.distributed users or loading model parallel models, set environment variables RANK, WORLD_SIZE and LOCAL_RANK.\n",
      "/home/qianq/mycodes/VisualGLM-6B/venv/lib/python3.10/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "[2023-10-24 15:33:14,803] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 7802193408\n",
      "[2023-10-24 15:33:16,349] [INFO] [RANK 0] global rank 0 is loading checkpoint /home/qianq/.sat_models/visualglm-6b/1/mp_rank_00_model_states.pt\n",
      "[2023-10-24 15:33:24,244] [INFO] [RANK 0] Will continue but found unexpected_keys! Check whether you are loading correct checkpoints: ['transformer.position_embeddings.weight'].\n",
      "[2023-10-24 15:33:24,249] [INFO] [RANK 0] > successfully loaded /home/qianq/.sat_models/visualglm-6b/1/mp_rank_00_model_states.pt\n"
     ]
    }
   ],
   "source": [
    "model, model_args = VisualGLMModel.from_pretrained(\n",
    "    name=\"visualglm-6b\",\n",
    "    args=argparse.Namespace(\n",
    "        fp16=True,\n",
    "        skip_init=True,\n",
    "        use_gpu_initialization=True,\n",
    "        device='cuda',\n",
    "    )\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_children_struct(model):\n",
    "    for item in model.named_children():\n",
    "        print('-----------------')\n",
    "        print(item[0])\n",
    "        print(item[1])\n",
    "        print('-----------------')\n",
    "\n",
    "def count_model_parameters(model):\n",
    "    total = sum([param.nelement() for param in model.parameters()])\n",
    "    print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "mixins\n",
      "ModuleDict(\n",
      "  (chatglm-final): ChatGLMFinalMixin(\n",
      "    (lm_head): ColumnParallelLinear()\n",
      "  )\n",
      "  (chatglm-attn): ChatGLMAttnMixin(\n",
      "    (rotary_emb): RotaryEmbedding()\n",
      "  )\n",
      "  (chatglm-layer): ChatGLMLayerMixin()\n",
      "  (eva): ImageMixin(\n",
      "    (model): BLIP2(\n",
      "      (vit): EVAViT(\n",
      "        (mixins): ModuleDict(\n",
      "          (patch_embedding): ImagePatchEmbeddingMixin(\n",
      "            (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "          )\n",
      "          (pos_embedding): InterpolatedPositionEmbeddingMixin()\n",
      "          (cls): LNFinalyMixin(\n",
      "            (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (transformer): BaseTransformer(\n",
      "          (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (word_embeddings): Embedding(1, 1408)\n",
      "          (position_embeddings): Embedding(257, 1408)\n",
      "          (layers): ModuleList(\n",
      "            (0-38): 39 x BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (qformer): QFormer(\n",
      "        (mixins): ModuleDict()\n",
      "        (transformer): BaseTransformer(\n",
      "          (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (word_embeddings): Embedding(32, 768)\n",
      "          (position_embeddings): None\n",
      "          (layers): ModuleList(\n",
      "            (0): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (final_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (glm_proj): Linear(in_features=768, out_features=4096, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "transformer\n",
      "BaseTransformer(\n",
      "  (embedding_dropout): Dropout(p=0, inplace=False)\n",
      "  (word_embeddings): VocabParallelEmbedding()\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x BaseTransformerLayer(\n",
      "      (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): SelfAttention(\n",
      "        (query_key_value): ColumnParallelLinear()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "        (dense): RowParallelLinear()\n",
      "        (output_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (dense_h_to_4h): ColumnParallelLinear()\n",
      "        (dense_4h_to_h): RowParallelLinear()\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "show_children_struct(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "chatglm-final\n",
      "ChatGLMFinalMixin(\n",
      "  (lm_head): ColumnParallelLinear()\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "chatglm-attn\n",
      "ChatGLMAttnMixin(\n",
      "  (rotary_emb): RotaryEmbedding()\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "chatglm-layer\n",
      "ChatGLMLayerMixin()\n",
      "-----------------\n",
      "-----------------\n",
      "eva\n",
      "ImageMixin(\n",
      "  (model): BLIP2(\n",
      "    (vit): EVAViT(\n",
      "      (mixins): ModuleDict(\n",
      "        (patch_embedding): ImagePatchEmbeddingMixin(\n",
      "          (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "        )\n",
      "        (pos_embedding): InterpolatedPositionEmbeddingMixin()\n",
      "        (cls): LNFinalyMixin(\n",
      "          (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer): BaseTransformer(\n",
      "        (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (word_embeddings): Embedding(1, 1408)\n",
      "        (position_embeddings): Embedding(257, 1408)\n",
      "        (layers): ModuleList(\n",
      "          (0-38): 39 x BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (qformer): QFormer(\n",
      "      (mixins): ModuleDict()\n",
      "      (transformer): BaseTransformer(\n",
      "        (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (word_embeddings): Embedding(32, 768)\n",
      "        (position_embeddings): None\n",
      "        (layers): ModuleList(\n",
      "          (0): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (glm_proj): Linear(in_features=768, out_features=4096, bias=True)\n",
      "  )\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "mixins = model.mixins\n",
    "show_children_struct(mixins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "model\n",
      "BLIP2(\n",
      "  (vit): EVAViT(\n",
      "    (mixins): ModuleDict(\n",
      "      (patch_embedding): ImagePatchEmbeddingMixin(\n",
      "        (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "      )\n",
      "      (pos_embedding): InterpolatedPositionEmbeddingMixin()\n",
      "      (cls): LNFinalyMixin(\n",
      "        (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (transformer): BaseTransformer(\n",
      "      (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (word_embeddings): Embedding(1, 1408)\n",
      "      (position_embeddings): Embedding(257, 1408)\n",
      "      (layers): ModuleList(\n",
      "        (0-38): 39 x BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qformer): QFormer(\n",
      "    (mixins): ModuleDict()\n",
      "    (transformer): BaseTransformer(\n",
      "      (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (word_embeddings): Embedding(32, 768)\n",
      "      (position_embeddings): None\n",
      "      (layers): ModuleList(\n",
      "        (0): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (glm_proj): Linear(in_features=768, out_features=4096, bias=True)\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "eva = model.mixins.eva\n",
    "show_children_struct(eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 1094.26M\n"
     ]
    }
   ],
   "source": [
    "count_model_parameters(eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 985.95M\n",
      "Number of parameter: 105.16M\n"
     ]
    }
   ],
   "source": [
    "vit = eva.model.vit\n",
    "qformer = eva.model.qformer\n",
    "count_model_parameters(vit)\n",
    "count_model_parameters(qformer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
