{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-12 18:47:11,214] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qianq/mycodes/VisualGLM-6B/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[2023-12-12 18:47:12,578] [WARNING] Failed to load bitsandbytes:No module named 'bitsandbytes'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from model.visualglm import VisualGLMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-12 18:47:12,617] [INFO] building VisualGLMModel model ...\n",
      "[2023-12-12 18:47:12,645] [INFO] [RANK 0] > initializing model parallel with size 1\n",
      "[2023-12-12 18:47:12,647] [INFO] [RANK 0] You are using model-only mode.\n",
      "For torch.distributed users or loading model parallel models, set environment variables RANK, WORLD_SIZE and LOCAL_RANK.\n",
      "/home/qianq/mycodes/VisualGLM-6B/venv/lib/python3.10/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "[2023-12-12 18:47:21,535] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 7802201600\n"
     ]
    }
   ],
   "source": [
    "model, model_args = VisualGLMModel.from_pretrained(\n",
    "    name=\"visualglm-6b\",\n",
    "    args=argparse.Namespace(\n",
    "        fp16=True,\n",
    "        skip_init=True,\n",
    "        use_gpu_initialization=True,\n",
    "        device='cuda',\n",
    "        cls_fusion=True,\n",
    "    ),\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_children_struct(model):\n",
    "    for item in model.named_children():\n",
    "        print('-----------------')\n",
    "        print(item[0])\n",
    "        print(item[1])\n",
    "        print('-----------------')\n",
    "\n",
    "def count_model_parameters(model):\n",
    "    total = sum([param.nelement() for param in model.parameters()])\n",
    "    print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "mixins\n",
      "ModuleDict(\n",
      "  (chatglm-final): ChatGLMFinalMixin(\n",
      "    (lm_head): ColumnParallelLinear()\n",
      "  )\n",
      "  (chatglm-attn): ChatGLMAttnMixin(\n",
      "    (rotary_emb): RotaryEmbedding()\n",
      "  )\n",
      "  (chatglm-layer): ChatGLMLayerMixin()\n",
      "  (eva): ImageMixin(\n",
      "    (model): BLIP2(\n",
      "      (vit): EVAViT(\n",
      "        (mixins): ModuleDict(\n",
      "          (patch_embedding): ImagePatchEmbeddingMixin(\n",
      "            (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "          )\n",
      "          (pos_embedding): InterpolatedPositionEmbeddingMixin()\n",
      "          (cls): LNFinalyMixin(\n",
      "            (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (transformer): BaseTransformer(\n",
      "          (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (word_embeddings): Embedding(1, 1408)\n",
      "          (position_embeddings): Embedding(257, 1408)\n",
      "          (layers): ModuleList(\n",
      "            (0-38): 39 x BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (qformer): QFormer(\n",
      "        (mixins): ModuleDict()\n",
      "        (transformer): BaseTransformer(\n",
      "          (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (word_embeddings): Embedding(32, 768)\n",
      "          (position_embeddings): None\n",
      "          (layers): ModuleList(\n",
      "            (0): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (cross_attention): CrossAttention(\n",
      "                (query): ColumnParallelLinear()\n",
      "                (key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): BaseTransformerLayer(\n",
      "              (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): SelfAttention(\n",
      "                (query_key_value): ColumnParallelLinear()\n",
      "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (dense): RowParallelLinear()\n",
      "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): ColumnParallelLinear()\n",
      "                (dense_4h_to_h): RowParallelLinear()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (final_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (glm_proj): Linear(in_features=768, out_features=4096, bias=True)\n",
      "    )\n",
      "    (fusion_model): FusionModel(\n",
      "      (embedding): Embedding(2, 4096)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "transformer\n",
      "BaseTransformer(\n",
      "  (embedding_dropout): Dropout(p=0, inplace=False)\n",
      "  (word_embeddings): VocabParallelEmbedding()\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x BaseTransformerLayer(\n",
      "      (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): SelfAttention(\n",
      "        (query_key_value): ColumnParallelLinear()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "        (dense): RowParallelLinear()\n",
      "        (output_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (dense_h_to_4h): ColumnParallelLinear()\n",
      "        (dense_4h_to_h): RowParallelLinear()\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "show_children_struct(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "chatglm-final\n",
      "ChatGLMFinalMixin(\n",
      "  (lm_head): ColumnParallelLinear()\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "chatglm-attn\n",
      "ChatGLMAttnMixin(\n",
      "  (rotary_emb): RotaryEmbedding()\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "chatglm-layer\n",
      "ChatGLMLayerMixin()\n",
      "-----------------\n",
      "-----------------\n",
      "eva\n",
      "ImageMixin(\n",
      "  (model): BLIP2(\n",
      "    (vit): EVAViT(\n",
      "      (mixins): ModuleDict(\n",
      "        (patch_embedding): ImagePatchEmbeddingMixin(\n",
      "          (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "        )\n",
      "        (pos_embedding): InterpolatedPositionEmbeddingMixin()\n",
      "        (cls): LNFinalyMixin(\n",
      "          (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer): BaseTransformer(\n",
      "        (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (word_embeddings): Embedding(1, 1408)\n",
      "        (position_embeddings): Embedding(257, 1408)\n",
      "        (layers): ModuleList(\n",
      "          (0-38): 39 x BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (qformer): QFormer(\n",
      "      (mixins): ModuleDict()\n",
      "      (transformer): BaseTransformer(\n",
      "        (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (word_embeddings): Embedding(32, 768)\n",
      "        (position_embeddings): None\n",
      "        (layers): ModuleList(\n",
      "          (0): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (cross_attention): CrossAttention(\n",
      "              (query): ColumnParallelLinear()\n",
      "              (key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BaseTransformerLayer(\n",
      "            (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (attention): SelfAttention(\n",
      "              (query_key_value): ColumnParallelLinear()\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (dense): RowParallelLinear()\n",
      "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (dense_h_to_4h): ColumnParallelLinear()\n",
      "              (dense_4h_to_h): RowParallelLinear()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (glm_proj): Linear(in_features=768, out_features=4096, bias=True)\n",
      "  )\n",
      "  (fusion_model): FusionModel(\n",
      "    (embedding): Embedding(2, 4096)\n",
      "  )\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "mixins = model.mixins\n",
    "show_children_struct(mixins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "model\n",
      "BLIP2(\n",
      "  (vit): EVAViT(\n",
      "    (mixins): ModuleDict(\n",
      "      (patch_embedding): ImagePatchEmbeddingMixin(\n",
      "        (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "      )\n",
      "      (pos_embedding): InterpolatedPositionEmbeddingMixin()\n",
      "      (cls): LNFinalyMixin(\n",
      "        (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (transformer): BaseTransformer(\n",
      "      (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (word_embeddings): Embedding(1, 1408)\n",
      "      (position_embeddings): Embedding(257, 1408)\n",
      "      (layers): ModuleList(\n",
      "        (0-38): 39 x BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qformer): QFormer(\n",
      "    (mixins): ModuleDict()\n",
      "    (transformer): BaseTransformer(\n",
      "      (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (word_embeddings): Embedding(32, 768)\n",
      "      (position_embeddings): None\n",
      "      (layers): ModuleList(\n",
      "        (0): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (cross_attention): CrossAttention(\n",
      "            (query): ColumnParallelLinear()\n",
      "            (key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_cross_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BaseTransformerLayer(\n",
      "          (input_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): ColumnParallelLinear()\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): RowParallelLinear()\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): ColumnParallelLinear()\n",
      "            (dense_4h_to_h): RowParallelLinear()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (glm_proj): Linear(in_features=768, out_features=4096, bias=True)\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "fusion_model\n",
      "FusionModel(\n",
      "  (embedding): Embedding(2, 4096)\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "eva = model.mixins.eva\n",
    "show_children_struct(eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 1094.27M\n"
     ]
    }
   ],
   "source": [
    "count_model_parameters(eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 985.95M\n",
      "Number of parameter: 105.16M\n"
     ]
    }
   ],
   "source": [
    "vit = eva.model.vit\n",
    "qformer = eva.model.qformer\n",
    "count_model_parameters(vit)\n",
    "count_model_parameters(qformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "mixins\n",
      "ModuleDict(\n",
      "  (patch_embedding): ImagePatchEmbeddingMixin(\n",
      "    (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "  )\n",
      "  (pos_embedding): InterpolatedPositionEmbeddingMixin()\n",
      "  (cls): LNFinalyMixin(\n",
      "    (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "transformer\n",
      "BaseTransformer(\n",
      "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (word_embeddings): Embedding(1, 1408)\n",
      "  (position_embeddings): Embedding(257, 1408)\n",
      "  (layers): ModuleList(\n",
      "    (0-38): 39 x BaseTransformerLayer(\n",
      "      (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "      (attention): SelfAttention(\n",
      "        (query_key_value): ColumnParallelLinear()\n",
      "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (dense): RowParallelLinear()\n",
      "        (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (dense_h_to_4h): ColumnParallelLinear()\n",
      "        (dense_4h_to_h): RowParallelLinear()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "show_children_struct(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "embedding_dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "-----------------\n",
      "-----------------\n",
      "word_embeddings\n",
      "Embedding(1, 1408)\n",
      "-----------------\n",
      "-----------------\n",
      "position_embeddings\n",
      "Embedding(257, 1408)\n",
      "-----------------\n",
      "-----------------\n",
      "layers\n",
      "ModuleList(\n",
      "  (0-38): 39 x BaseTransformerLayer(\n",
      "    (input_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "    (attention): SelfAttention(\n",
      "      (query_key_value): ColumnParallelLinear()\n",
      "      (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (dense): RowParallelLinear()\n",
      "      (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (post_attention_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLP(\n",
      "      (dense_h_to_4h): ColumnParallelLinear()\n",
      "      (dense_4h_to_h): RowParallelLinear()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "vit_transformer = vit.transformer\n",
    "show_children_struct(vit_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"/home/qianq/model/chatglm-6b\", trust_remote_code=True)\n",
    "# AutoModel.from_pretrained(\"/home/qianq/model/chatglm-6b\", trust_remote_code=True, from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "transformer\n",
      "ChatGLMModel(\n",
      "  (word_embeddings): Embedding(130528, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x GLMBlock(\n",
      "      (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): SelfAttention(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "        (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      )\n",
      "      (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GLU(\n",
      "        (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "-----------------\n",
      "-----------------\n",
      "lm_head\n",
      "Linear(in_features=4096, out_features=130528, bias=False)\n",
      "-----------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "show_children_struct(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
